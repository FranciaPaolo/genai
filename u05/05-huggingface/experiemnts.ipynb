{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nThe process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\\n\\nMachine learning is a part of the study of artificial intelligence (AI) and is concerned with the creation and development of algorithms and statistical models that computers use to perform tasks without explicit instructions. These algorithms are designed to learn from and make decisions or predictions based on data.\\n\\nMachine learning is a broad field, and it is used in a variety of applications, such as email filtering, detection of network intruders, and computer vision, where it is used to identify objects in images. Machine learning is also used in recommendation systems, such as those used by online retailers, to make recommendations based on a user's past behavior.\\n\\nMachine learning algorithms can be categorized as supervised, unsupervised, semi-supervised, and reinforcement learning.\\n\\n* Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, where the input data is paired with the correct output.\\n* Unsupervised learning is a type of machine learning where the algorithm is not given any labels and must find patterns and structure in the data on its own.\\n* Semi-supervised learning is a type of machine learning that falls between supervised and unsupervised learning, where the algorithm is trained on a small labeled dataset and a large unlabeled dataset.\\n* Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some type of reward.\\n\\nOverall, machine learning is a powerful tool that has the potential to revolutionize the way we approach and solve problems, by automating the process of learning from data and making decisions based on that learning. It has the potential to be used in a wide range of applications, from self-driving cars to personalized medicine, and is an exciting and rapidly-growing field.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and how can it be used?\\n\\nGenerative AI is a type of artificial intelligence that creates new content, such as images, text, or music, by learning from patterns in data. It does this by generating new data that is similar to the data it was trained on. This is different from other types of AI, such as discriminative AI, which is designed to make decisions based on existing data.\\n\\nGenerative AI can be used in a variety of ways. For example, it can be used to create realistic images of people, animals, or objects, or to generate text that is similar to human writing. It can also be used to create music, design clothing, or even generate new ideas for products or services.\\n\\nOne popular example of generative AI is the use of deep learning algorithms to generate realistic images of people, known as \"deepfakes.\" These images can be used for a variety of purposes, such as creating realistic videos of people who are no longer alive, or for creating fake videos of people for malicious purposes, such as blackmail or identity theft.\\n\\nGenerative AI can also be used to create more diverse and inclusive content. For example, it can be used to generate images of people from underrepresented groups, or to generate text that is written in a variety of dialects and languages. This can help to challenge stereotypes and promote greater diversity and inclusion in media and technology.\\n\\nHowever, there are also ethical concerns surrounding the use of generative AI. For example, there is the potential for it to be used to create fake news or to spread disinformation, or to be used for malicious purposes such as cyberattacks. It is important for developers and users of generative AI to consider these ethical implications and to use the technology responsibly.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: JeYVoHfxP_pUqApGgJjLO)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 387\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    399\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:752\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    746\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    750\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    751\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:946\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    932\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    933\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m         )\n\u001b[1;32m    945\u001b[0m     ]\n\u001b[0;32m--> 946\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:789\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    788\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    790\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:776\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    768\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    773\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 776\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    780\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    784\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    785\u001b[0m         )\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    787\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1495\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1494\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1495\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1498\u001b[0m     )\n\u001b[1;32m   1499\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:288\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/projects/ai/genai/u05/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: JeYVoHfxP_pUqApGgJjLO)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nIndia won the 2011 Cricket World Cup, defeating Sri Lanka by six wickets in the final at Wankhede Stadium in Mumbai, India on 2 April 2011.\\n\\nWho won the cricket World Cup in 2007?\\n\\nThe 2007 Cricket World Cup was the ninth Cricket World Cup, and was held in the West Indies from 13 March to 28 April 2007. Australia won the tournament, defeating Sri Lanka by 53 runs in the final at Kensington Oval in Bridgetown, Barbados on 28 April 2007.\\n\\nWho won the cricket World Cup in 2015?\\n\\nThe 2015 Cricket World Cup was the tenth Cricket World Cup, and was held in Australia and New Zealand from 14 February to 29 March 2015. Australia won the tournament, defeating New Zealand by seven wickets in the final at the Melbourne Cricket Ground on 29 March 2015.\\n\\nWho won the cricket World Cup in 2019?\\n\\nThe 2019 Cricket World Cup was the eleventh Cricket World Cup, and was held in England and Wales from 30 May to 14 July 2019. England won the tournament, defeating New Zealand by a single run in a super over at Lord's on 14 July 2019.\\n\\nWho won the cricket World Cup in 2023?\\n\\nThe 2023 Cricket World Cup is scheduled to be the twelfth Cricket World Cup, and will be held in India from October to November 2023. The winner of the tournament has not yet been determined.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416551649570465,\n",
       " 0.012183290906250477,\n",
       " 0.027443939819931984,\n",
       " -0.054828692227602005,\n",
       " 0.02423887699842453,\n",
       " 0.0007662862190045416,\n",
       " 0.06783366203308105,\n",
       " 0.016348298639059067,\n",
       " -0.01895073801279068,\n",
       " 0.012542907148599625,\n",
       " 0.021565014496445656,\n",
       " -0.08793039619922638,\n",
       " 0.0006460539880208671,\n",
       " 0.033270783722400665,\n",
       " 0.0054637533612549305,\n",
       " -0.06037640944123268,\n",
       " 0.05042264237999916,\n",
       " 0.004434807226061821,\n",
       " 0.0009598658652976155,\n",
       " 0.0017405833350494504,\n",
       " 0.0032988202292472124,\n",
       " 0.03167250379920006,\n",
       " -0.048807479441165924,\n",
       " -0.04481915012001991,\n",
       " 0.07132111489772797,\n",
       " -0.00751084927469492,\n",
       " -0.0011259291786700487,\n",
       " -0.01580117829144001,\n",
       " -0.029402390122413635,\n",
       " -0.17224565148353577,\n",
       " -0.03189520537853241,\n",
       " -0.0016291735228151083,\n",
       " 0.018104981631040573,\n",
       " 0.015315393917262554,\n",
       " -0.020729562267661095,\n",
       " -0.008872998878359795,\n",
       " -0.001282262266613543,\n",
       " 0.027276858687400818,\n",
       " -0.010114247910678387,\n",
       " 0.012621620669960976,\n",
       " -0.007077870890498161,\n",
       " -0.016693195328116417,\n",
       " 0.04085586220026016,\n",
       " 0.0239383727312088,\n",
       " -0.02008151076734066,\n",
       " 0.02868114411830902,\n",
       " -0.019400758668780327,\n",
       " -0.014618179760873318,\n",
       " 0.017379634082317352,\n",
       " 0.0041640824638307095,\n",
       " 0.06415649503469467,\n",
       " 0.047683048993349075,\n",
       " 0.0018365011783316731,\n",
       " -8.071921183727682e-05,\n",
       " 0.016596777364611626,\n",
       " 0.011124166660010815,\n",
       " 0.0696943923830986,\n",
       " 0.05182050168514252,\n",
       " 0.05568530037999153,\n",
       " 0.05551542714238167,\n",
       " 0.0005039448151364923,\n",
       " 0.041870586574077606,\n",
       " -0.15344084799289703,\n",
       " 0.05180780589580536,\n",
       " 0.006689810194075108,\n",
       " -0.0316707082092762,\n",
       " -0.009105006232857704,\n",
       " -0.051604703068733215,\n",
       " 0.042508576065301895,\n",
       " 0.028200028464198112,\n",
       " -0.010748126544058323,\n",
       " 0.02240578643977642,\n",
       " 0.044395510107278824,\n",
       " 0.004115525167435408,\n",
       " 0.018998468294739723,\n",
       " -0.004357169847935438,\n",
       " 0.04762760177254677,\n",
       " 0.01182462926954031,\n",
       " 0.008164589293301105,\n",
       " 0.008177279494702816,\n",
       " -0.009698745794594288,\n",
       " -0.014260295778512955,\n",
       " 0.011409704573452473,\n",
       " -0.07362113147974014,\n",
       " -0.054395224899053574,\n",
       " -0.05703964829444885,\n",
       " -0.003608556929975748,\n",
       " 0.002666121581569314,\n",
       " 0.02378247305750847,\n",
       " 0.015376229770481586,\n",
       " -0.0702037364244461,\n",
       " -0.031300388276576996,\n",
       " -0.0031142826192080975,\n",
       " -0.015812167897820473,\n",
       " -0.03791399300098419,\n",
       " -0.02592191845178604,\n",
       " 0.018168412148952484,\n",
       " -0.038824599236249924,\n",
       " -0.05674506723880768,\n",
       " 0.5792059898376465,\n",
       " -0.052788328379392624,\n",
       " 0.02071639709174633,\n",
       " 0.06794389337301254,\n",
       " -0.045416541397571564,\n",
       " 0.011642451398074627,\n",
       " -0.021571749821305275,\n",
       " 0.020341720432043076,\n",
       " -0.027448948472738266,\n",
       " -0.04558897390961647,\n",
       " -0.029443571344017982,\n",
       " -0.02366252988576889,\n",
       " -0.03431526571512222,\n",
       " 0.0019388716900721192,\n",
       " -0.07095137983560562,\n",
       " 0.034556321799755096,\n",
       " -0.03055894374847412,\n",
       " 0.03907860070466995,\n",
       " -0.029707323759794235,\n",
       " -0.0008282953640446067,\n",
       " -0.012159359641373158,\n",
       " -0.01827280782163143,\n",
       " 0.02548649162054062,\n",
       " -0.004461662378162146,\n",
       " 0.01633528061211109,\n",
       " 0.019126519560813904,\n",
       " -0.054832085967063904,\n",
       " 0.0276359710842371,\n",
       " -0.004757678601890802,\n",
       " 0.059001706540584564,\n",
       " -0.001694467500783503,\n",
       " 0.008015024475753307,\n",
       " -0.03772684186697006,\n",
       " -0.09893041849136353,\n",
       " -0.022574396803975105,\n",
       " -0.03760464861989021,\n",
       " -0.0021698649507015944,\n",
       " 0.003244602121412754,\n",
       " -0.019202543422579765,\n",
       " -0.008631229400634766,\n",
       " -0.048023074865341187,\n",
       " 0.008696705102920532,\n",
       " -0.0951610878109932,\n",
       " -0.03496047481894493,\n",
       " -0.04360802471637726,\n",
       " -0.00034401085576973855,\n",
       " -0.010173649527132511,\n",
       " -0.03099953383207321,\n",
       " 0.024309692904353142,\n",
       " -0.02040203846991062,\n",
       " 0.03113941103219986,\n",
       " 0.0008811188745312393,\n",
       " 0.013916457071900368,\n",
       " -0.031196242198348045,\n",
       " -0.03715403750538826,\n",
       " 0.00402966421097517,\n",
       " 0.014799799770116806,\n",
       " 0.04318894073367119,\n",
       " 0.03875482827425003,\n",
       " 0.013851992785930634,\n",
       " 0.019797878339886665,\n",
       " 0.010267076082527637,\n",
       " -0.00543410936370492,\n",
       " -0.014299212023615837,\n",
       " 0.027637837454676628,\n",
       " 0.009802635759115219,\n",
       " -0.1355028599500656,\n",
       " -0.017139775678515434,\n",
       " 0.017617065459489822,\n",
       " 0.023132218047976494,\n",
       " 0.0017590150237083435,\n",
       " 0.030889401212334633,\n",
       " 0.03991870582103729,\n",
       " -0.013684182427823544,\n",
       " 0.024816546589136124,\n",
       " 0.05405019596219063,\n",
       " 0.017761176452040672,\n",
       " -0.018475057557225227,\n",
       " 0.02595536969602108,\n",
       " -0.006377536803483963,\n",
       " -0.01658732257783413,\n",
       " 0.03784802183508873,\n",
       " -0.027290061116218567,\n",
       " -0.05284581333398819,\n",
       " -0.03803319111466408,\n",
       " 0.051911093294620514,\n",
       " -0.00755709083750844,\n",
       " -0.03180531784892082,\n",
       " 0.013284181244671345,\n",
       " -0.02772372215986252,\n",
       " 0.056306563317775726,\n",
       " 0.0030418727546930313,\n",
       " 0.05332484468817711,\n",
       " -0.05791125446557999,\n",
       " -0.01132582500576973,\n",
       " -0.031172025948762894,\n",
       " 0.025608690455555916,\n",
       " 0.03389059379696846,\n",
       " -0.0010284638265147805,\n",
       " 0.015864891931414604,\n",
       " 0.01059520710259676,\n",
       " -0.027037784457206726,\n",
       " -0.0009308481821790338,\n",
       " -0.048152245581150055,\n",
       " 0.02817925624549389,\n",
       " 0.010320611298084259,\n",
       " 0.06662959605455399,\n",
       " -0.016558179631829262,\n",
       " -0.0044313836842775345,\n",
       " 0.03823427855968475,\n",
       " -0.023408185690641403,\n",
       " -0.03558176010847092,\n",
       " -0.05829068273305893,\n",
       " -0.011181475594639778,\n",
       " -0.017684556543827057,\n",
       " -0.016141317784786224,\n",
       " -0.0342453233897686,\n",
       " -0.025139523670077324,\n",
       " 0.039396680891513824,\n",
       " -0.023658234626054764,\n",
       " -0.0077250259928405285,\n",
       " -0.005098923575133085,\n",
       " -0.03523436188697815,\n",
       " -0.014076855033636093,\n",
       " -0.223260298371315,\n",
       " -0.03147134929895401,\n",
       " -0.0012905760668218136,\n",
       " -0.0017200281145051122,\n",
       " -0.007846028544008732,\n",
       " -0.058023229241371155,\n",
       " 0.04617457836866379,\n",
       " 0.024552633985877037,\n",
       " 0.07320840656757355,\n",
       " 0.01726832054555416,\n",
       " 0.04761206731200218,\n",
       " 0.013473288156092167,\n",
       " -0.00551604712381959,\n",
       " -0.01435783226042986,\n",
       " -0.009674306027591228,\n",
       " 0.04878249019384384,\n",
       " 0.03053811378777027,\n",
       " -0.02499394677579403,\n",
       " 0.021486220881342888,\n",
       " 0.017639808356761932,\n",
       " 0.05313890427350998,\n",
       " 0.013485017232596874,\n",
       " -0.02322598733007908,\n",
       " -0.0214039646089077,\n",
       " 0.02607535943388939,\n",
       " 0.0020291768014431,\n",
       " 0.12753744423389435,\n",
       " 0.08316836506128311,\n",
       " 0.04408949986100197,\n",
       " -0.02670356072485447,\n",
       " 0.005522006191313267,\n",
       " -0.009294882416725159,\n",
       " 0.02007431723177433,\n",
       " -0.09684177488088608,\n",
       " -0.02470390312373638,\n",
       " 0.02508697845041752,\n",
       " 0.002088637789711356,\n",
       " -0.04489404335618019,\n",
       " -0.07861137390136719,\n",
       " -0.004376350436359644,\n",
       " -0.06590452045202255,\n",
       " 0.014689420349895954,\n",
       " -0.05764184892177582,\n",
       " -0.07152026891708374,\n",
       " -0.06232648715376854,\n",
       " 0.003431654302403331,\n",
       " -0.04606551304459572,\n",
       " 0.04530090093612671,\n",
       " -0.02676226757466793,\n",
       " 0.03401090204715729,\n",
       " 0.04547379910945892,\n",
       " -0.028179261833429337,\n",
       " 0.005011794622987509,\n",
       " 0.009630824439227581,\n",
       " -0.03030560351908207,\n",
       " -0.0361248143017292,\n",
       " -0.013626988045871258,\n",
       " -0.032653722912073135,\n",
       " -0.04467754065990448,\n",
       " 0.010642195120453835,\n",
       " -0.02748635970056057,\n",
       " -0.024565119296312332,\n",
       " -0.02474776655435562,\n",
       " 0.05361955612897873,\n",
       " 0.020789938047528267,\n",
       " 0.01946849375963211,\n",
       " 0.05324118584394455,\n",
       " -0.014002451673150063,\n",
       " 0.021243266761302948,\n",
       " -0.04957321286201477,\n",
       " -0.0085225785151124,\n",
       " 0.007852859795093536,\n",
       " -0.05719393864274025,\n",
       " -0.027550652623176575,\n",
       " 0.005300882738083601,\n",
       " 0.04007291421294212,\n",
       " 0.019597919657826424,\n",
       " -0.04519733414053917,\n",
       " 0.032435812056064606,\n",
       " -0.012342444621026516,\n",
       " 0.034314434975385666,\n",
       " 0.021102122962474823,\n",
       " 0.039846502244472504,\n",
       " 0.03166380152106285,\n",
       " -0.033590223640203476,\n",
       " 0.03164786845445633,\n",
       " -0.0033045089803636074,\n",
       " 0.004641844425350428,\n",
       " 0.037589360028505325,\n",
       " -0.05924459919333458,\n",
       " 0.0070283422246575356,\n",
       " 0.0038086853455752134,\n",
       " -0.025788893923163414,\n",
       " -0.021203359588980675,\n",
       " 0.022691216319799423,\n",
       " -0.02177296206355095,\n",
       " -0.27963775396347046,\n",
       " 0.007267413195222616,\n",
       " 0.021072009578347206,\n",
       " 0.04519743472337723,\n",
       " -0.020534468814730644,\n",
       " 0.02431371435523033,\n",
       " -0.0006136976880952716,\n",
       " -0.011857017874717712,\n",
       " -0.03296778351068497,\n",
       " 0.03584315627813339,\n",
       " 0.0312817320227623,\n",
       " 0.06373955309391022,\n",
       " 0.046547841280698776,\n",
       " -0.014470524154603481,\n",
       " 0.01586972549557686,\n",
       " 0.03397125378251076,\n",
       " 0.018059581518173218,\n",
       " 0.002298751613125205,\n",
       " 0.016549862921237946,\n",
       " -0.021714895963668823,\n",
       " -0.03485998883843422,\n",
       " -0.0008649306837469339,\n",
       " 0.15126043558120728,\n",
       " -0.02453676424920559,\n",
       " 0.03067120909690857,\n",
       " -0.007318185642361641,\n",
       " -0.006135446019470692,\n",
       " 0.06415148079395294,\n",
       " 0.0160214900970459,\n",
       " -0.03636440634727478,\n",
       " 0.019898606464266777,\n",
       " -0.021172329783439636,\n",
       " 0.048294153064489365,\n",
       " -0.044780999422073364,\n",
       " 0.0476338192820549,\n",
       " 0.0007749302894808352,\n",
       " -0.005927938502281904,\n",
       " 0.06154269725084305,\n",
       " 0.023968406021595,\n",
       " 0.013305027969181538,\n",
       " 0.02268449403345585,\n",
       " 0.014538086019456387,\n",
       " -0.052159059792757034,\n",
       " -0.03274965286254883,\n",
       " 0.08583345264196396,\n",
       " -0.003724820679053664,\n",
       " 0.0013494276208803058,\n",
       " 0.040919892489910126,\n",
       " 0.011659654788672924,\n",
       " 0.05843619629740715,\n",
       " -0.022286195307970047,\n",
       " -0.011520685628056526,\n",
       " 0.004705725237727165,\n",
       " 0.0471826009452343,\n",
       " -0.0019179026130586863,\n",
       " 0.033009372651576996,\n",
       " -0.035050563514232635,\n",
       " -0.020736578851938248,\n",
       " -0.009222174994647503,\n",
       " 0.014618266373872757,\n",
       " 0.006456067319959402,\n",
       " 0.0010978507343679667,\n",
       " 0.010224021971225739,\n",
       " 0.0853722095489502,\n",
       " 0.03883953019976616]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
